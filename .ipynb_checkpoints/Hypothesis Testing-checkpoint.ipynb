{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wstęp teoretyczny \n",
    "\n",
    "Hipoteza statystyczna to każde stwierdzenie dotyczące nieznanego rozkładu zmiennej losowej. Jeśli hipotetyzujemy parametry rozkładu, to hipozteza jest parametryczna, w przeciwnym wypadku jest nieparametryczna. Jeśli hipoteza w całości opisuje dany rozkład (np. przypuszczamy, że \\$ \\lambda = 2\\$ w rozkładzie Poissona), to hipoteza jest prosta. W przeciwnym wypadku (np. przypuszczamy, że \\$ \\lambda > 2\\$) hipoteza jest złożona. \n",
    "\n",
    "Logika jest następująca: wysuwamy hipotezę, zwaną hipotezą zerową \\$ H_0 \\$ oraz 'dopełniającą' ją hipotezę alternatywną \\$ H_1 \\$. Na podstawie próby chcemy wnioskować o populacji i mamy dwa scenariusze: \n",
    "1. Odrzucamy hipotezę, bo próba dostarczyła wystarczająco informacji\n",
    "2. Nie odrzucamy hipotezy, bo próba nie dostarczyła wystarczająco informacji \n",
    "\n",
    "Obieramy odpowiednią statystykę \\$ \\theta \\$ i konstruujemy obszar krytyczny \\$ R_{\\alpha} \\$.  \n",
    "\n",
    "Ustalamy z góry pewne małe prawdopodobieństwo, \\$ \\alpha \\$, które jest zwane poziomem istotności. Jest to prawdopodobieństwo popełnienia błędu I rodzaju, który polega na odrzuceniu hipotezy \\$ H_0 \\$, gdy jest ona prawdziwa.\n",
    "\n",
    "\\$ P(\\theta \\in R_{\\alpha} | H_0) = \\alpha \\$\n",
    "\n",
    "Jako że tylko odrzucenie hipotezy zerowej stawia nas w konkretnej sytuacji, to eksperyment tworzymy w ten sposób, by hipotezą zerową było stwierdzenie, co do którego fałszywości jesteśmy bardziej przekonani. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy dotyczące wartości oczekiwanej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Badanie wartości średniej populacji \n",
    "###### Model 1\n",
    "\n",
    "Założenia \n",
    "1. \\$ X \\sim N(\\mu, \\sigma) \\$\n",
    "2. \\$ \\sigma \\$ jest znane\n",
    "3. n, \\$ \\alpha \\$\n",
    "\n",
    "Stawiamy hipotezę\n",
    "\\$ H_0: \\mu = \\mu_0\\$\n",
    "\n",
    "1. \\$ H_1: \\mu \\neq \\mu_0\\$\n",
    "\\$ R_{\\alpha} = (- \\infty, u(\\frac{\\alpha}{2})) \\cup (u(1 - \\frac{\\alpha}{2}), \\infty)\\$\n",
    "\n",
    "2. \\$ H_1: \\mu \\le \\mu_0\\$\n",
    "\\$ R_{\\alpha} = (- \\infty, u(\\alpha)) \\$\n",
    "\n",
    "3. \\$ H_1: \\mu \\ge \\mu_0\\$\n",
    "\\$ R_{\\alpha} = (u(1 - \\alpha), \\infty)\\$\n",
    "\n",
    "W dwóch ostatnich przypadkach wybieramy przedziały jednostronne, bo wtedy moc testu (prawdopodobieństwo odrzucenia \\$ H_0 \\$, gdy \\$ H_1 \\$ jest prawdziwa) jest największa. \n",
    "\n",
    "Statystyką testową jest \n",
    "\n",
    "$$ U = \\frac{\\overline X - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}$$\n",
    "\n",
    "która ma rozkład w normalny. \n",
    "\n",
    "Przykład testu:\n",
    "Z populacji o rozkładzie normalnym \\$ N(\\mu, 1) \\$ pobrano próbę \\$ n = 16 \\$ elementów. Średnia wyszła 0.1, a naszą hipotezą jest\n",
    "\\$ H_0: \\mu = 0 \\$. Badać wobec hipotezy alternatywnej \\$ H_1: \\mu \\neq 0 \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "sample_mean = 0.1\n",
    "hypothesized_mean = 0\n",
    "pop_std = 1\n",
    "n = 16\n",
    "alpha = 0.01\n",
    "\n",
    "stat = (sample_mean - hypothesized_mean) / (pop_std / np.sqrt(n))\n",
    "\n",
    "q1 = norm.ppf(1 - alpha / 2)\n",
    "q2 = norm.ppf(alpha / 2)\n",
    "\n",
    "stat < q2 or stat > q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie mamy więc podstaw do odrzucenia hipotezy zerowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model 2\n",
    "\n",
    "Założenia \n",
    "1. \\$ X \\sim N(\\mu, \\sigma) \\$\n",
    "2. \\$ \\sigma \\$ jest nieznane\n",
    "3. n, \\$ \\alpha \\$\n",
    "\n",
    "Stawiamy hipotezę\n",
    "\\$ H_0: \\mu = \\mu_0\\$\n",
    "\n",
    "1. \\$ H_1: \\mu \\neq \\mu_0\\$\n",
    "\\$ R_{\\alpha} = (- \\infty, t(\\frac{\\alpha}{2}, n - 1)) \\cup (t(1 - \\frac{\\alpha}{2}, n - 1), \\infty)\\$\n",
    "\n",
    "2. \\$ H_1: \\mu \\le \\mu_0\\$\n",
    "\\$ R_{\\alpha} = (- \\infty, t(\\alpha, n - 1)) \\$\n",
    "\n",
    "3. \\$ H_1: \\mu \\ge \\mu_0\\$\n",
    "\\$ R_{\\alpha} = (t(1 - \\alpha, n - 1), \\infty)\\$\n",
    "\n",
    "$$ U = \\frac{\\overline X - \\mu}{S}\\sqrt{n - 1}$$\n",
    "\n",
    "która ma rozkład w t-Studenta o n - 1 stopniach swobody.\n",
    "\n",
    "Przykład testu:\n",
    "Z próby n = 10 elementów obliczono \\$ \\overline x = 4.8 \\$, \\$ s = 0.5 \\$. Na poziomie istotności \\$ \\alpha = 0.01 \\$ zweryfikować hipotezę\n",
    "\n",
    "\\$ H_0: \\mu = 5\\$\n",
    "wobec hipotezy\n",
    "\\$ H_1: \\mu \\neq 5 \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "sample_mean = 4.8\n",
    "hypothesized_mean = 5\n",
    "sample_std = 0.5\n",
    "n = 10\n",
    "alpha = 0.01\n",
    "\n",
    "tq = t.ppf(1 - alpha / 2, n - 1)\n",
    "\n",
    "t_stat = (sample_mean - hypothesized_mean) / sample_std * np.sqrt(n - 1)\n",
    "\n",
    "t_stat < - tq or t_stat > tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.200000000000001\n"
     ]
    }
   ],
   "source": [
    "print(t_stat)\n",
    "# nie ma podstaw do odrzucenia hipotezy, że średnia jest \n",
    "# równa 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Badanie równości średniej dwóch populacji - nieznane wariancje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wysunięto hipotezę, że stopien wyprania tkaniny wełnianej płatkami mydlanymi jest wyższy od stopnia wyprania sulfapolem. W tym celu pobrano próbkę o \\$n_1 = 10\\$ i wartościach 74.8, 75.1, 73.0, 72.8, 76.2, 74.6, 76.0, 73.4, 72.9, 71.6, oraz próbkę o \\$n_2 = 7\\$ i wartościach 56.9, 57.8, 54.6, 59.0, 57.1, 58.2, 57.6. Zakładając, że stopien wyprania tkaniny ma rozkład normalny, zweryfikować na poziomie \\$\\alpha = 0.05 \\$ hipotezę. \n",
    "\n",
    "Rozwiązanie \n",
    "\n",
    "- \\$H_0 = \\mu_1 = \\mu_2 \\$\n",
    "- \\$A = \\mu_1 > \\mu_2\\$\n",
    "\n",
    "gdzie przez \\$\\mu_1\\$ oznaczamy średni stopień wyprania tkanin wypranych płatkami mydlanymi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population 1: mean = 74.04, variance = 2.0804000000000005\n",
      "Population 2: mean = 57.31428571428571, variance = 1.6469387755102034\n"
     ]
    }
   ],
   "source": [
    "n1 = 10\n",
    "n2 = 7\n",
    "alpha = 0.05\n",
    "\n",
    "arr1 = np.array([74.8, 75.1, 73.0, 72.8, 76.2, 74.6, 76.0, 73.4, 72.9, 71.6])\n",
    "arr2 = np.array([56.9, 57.8, 54.6, 59.0, 57.1, 58.2, 57.6])\n",
    "\n",
    "mean1 = arr1.mean()\n",
    "mean2 = arr2.mean()\n",
    "\n",
    "var1 = arr1.var(ddof=0)\n",
    "var2 = arr2.var(ddof=0)\n",
    "\n",
    "print(f\"Population 1: mean = {mean1}, variance = {var1}\")\n",
    "print(f\"Population 2: mean = {mean2}, variance = {var2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie znamy odchyleń standardowych populacji, więc chcielibyśmy skorzystać ze statystyki \n",
    "\n",
    "\\$ t = \\frac{\\overline X_1 - \\overline X_2}{\\sqrt{\\frac{((n_1 - 1) S_1^{2} + (n_2 - 1) S_2^{2})(n_1 + n_2)}{(n_1 + n_2 - 2)(n_1 n_2)}}}\\$\n",
    "\n",
    "Jednak nie wiemy, czy \\$ \\sigma_1 = \\sigma_2 \\$.\n",
    "W tym celu wykonamy najpierw test równości wariancji. Bez wchodzenia w szczegóły testu (będzie o nim potem) obliczamy wartość statystyki rozkładu F\n",
    "\n",
    "\\$ F = \\frac{\\frac{n_1}{n_1 - 1}S_1}{\\frac{n_2}{n_2 - 1}S_2}\\$ \n",
    "\n",
    "Która przy prawdziwości hipotezy H_0 ma rozkład F Snedecora o (n_1 - 1, n_2 - 1) stopniach swobody \n",
    "\n",
    "\\$ F = max(F, F') = \\frac{\\max{S_1^{\\ast2} S_2^{\\ast2}}}{\\min{S_1^{\\ast2} S_2^{\\ast2}}}\\$\n",
    "\n",
    "\n",
    "Dla hipotezy \\$A = \\sigma_1 \\neq \\sigma_2\\$ zbiorem krytycznym jest \\$ <F(1 - \\frac{\\alpha}{2}, n_l - 1, n_m - 1); +\\infty) \\$\n",
    "\n",
    "gdzie \\$ n_l \\$ jest licznością dla której obliczono licznik a \\$ n_m \\$ dla której obliczono mianownik\n",
    "\n",
    "Warto zauważyć, że jeżeli zawsze bierzemy w liczniku większą wartość, to interesuje nas prawy ogon rozkładu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.070398264149241  >  5.523406623975584 ?\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f\n",
    "\n",
    "F = (n1 / (n1 - 1) * np.sqrt(var1)) / (n2 / (n2 - 1) * np.sqrt(var2))\n",
    "critical_set = f.ppf(1 - (alpha/2), n1 - 1, n2 - 1)\n",
    "print(F, \" > \", critical_set, \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b0fc22d280>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 5, 100)\n",
    "\n",
    "plt.plot(x, f.pdf(x, n1 - 1, n2 - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczona wartość F nie należy do zbioru krytycznego, więc nie możemy odrzucić hipotezy o równości wariancji (Ale też jej nie akceptujemy).\n",
    "\n",
    "W każdym razie przechodzimy do weryfikacji głównej hipotezy tego zadania. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.577183973799897\n"
     ]
    }
   ],
   "source": [
    "def t_stat(x, y):\n",
    "    x_mean, x_var = x.mean(), x.var(ddof=0)\n",
    "    y_mean, y_var = y.mean(), y.var(ddof=0)\n",
    "    n1, n2 = len(x), len(y)\n",
    "    \n",
    "    nominator = (x_mean - y_mean)\n",
    "    denominator = np.sqrt(((n1 - 1) * x_var + (n2 - 1) * y_var) / (n1 + n2 - 2) * (1 / n1 + 1 / n2))\n",
    "    \n",
    "    return nominator / denominator\n",
    "\n",
    "t_computed = t_stat(arr1, arr2)\n",
    "print(t_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako zbiór krytyczny przyjmujemy \\$<t(1 - \\alpha; n_1 - 1; n_2 - 1); \\infty)\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.833112932653633"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t \n",
    "\n",
    "critical_set = t.ppf(1 - alpha, n1 - 1, n2 - 1)\n",
    "critical_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_computed > critical_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy więc odrzucić hipotezę zerową na poziomie istotności 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Badanie równości średnich - metoda zmiennych połączonych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmierzono ciśnienie tętnicze wśród losowo wybranej grupy pacjentów przed i po podaniem leku. Otrzymano wyniki\n",
    "\n",
    "przed: 210, 180, 260, 270, 190, 250, 180\n",
    "\n",
    "po: 180, 160, 220, 260, 200, 230, 180\n",
    "\n",
    "Na poziomie istotności \\$ \\alpha = 0.05 \\$ zweryfikować, czy lek działa. \n",
    "\n",
    "Będziemy korzystać z modelu testu, gdzie pobieramy próbkę \\$x_1, ..., x_n\\$ z populacji o rozkładzie normalnym przed pewnym eksperymentem, a następnie próbkę \\$y_1, ..., y_n\\$ (w tej samej kolejności). \n",
    "Badamy zmienną \\$ Z = X - Y\\$, więc hipoteza \\$ \\mu_X - \\mu_Y = 0 => \\mu_Z = 0\\$\n",
    "\n",
    "Korzystamy ze statystyki \n",
    "\n",
    "\\$ t = \\frac{\\overline Z}{S_Z} \\sqrt{n - 1}\\$\n",
    "\n",
    "która ma rozkład t-Studenta o n - 1 stopniach swobody\n",
    "\n",
    "Stawiamy hipotezy:\n",
    "\n",
    "- \\$H_0 : \\mu_Z = 0\\$\n",
    "- \\$A : \\mu_Z > 0\\$\n",
    "\n",
    "przedziałem krytycznym jest \\$<t(1 - \\alpha; n - 1); \\infty)\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.419677397852329"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "X = np.array([210, 180, 260, 270, 190, 250, 180])\n",
    "Y = np.array([180, 160, 220, 260, 200, 230, 180])\n",
    "\n",
    "def t_paired(x, y):\n",
    "    z = x - y \n",
    "    n = len(z)\n",
    "    return (z.mean()) / (z.std(ddof=0)) * (np.sqrt(n - 1))\n",
    "\n",
    "t_computed = t_paired(X, Y)\n",
    "t_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6fa3af92410d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mt_computed\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "t_computed > t.ppf(1 - alpha, n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy więc odrzucić hipotezę zerową na rzecz hipotezy alternatywnej - wyniki badań przemawiają za działaniem leku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonano pomiary wytrzymałości betonu dla dwóch różnych receptur A i B. Otrzymano wyniki (niżej). Na poziomie istotności 0.01 zbadać, czy wytrzymałość zależy od receptury. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([20, 33, 35, 25, 30,27])\n",
    "B = np.array([28, 30, 33, 20, 24, 18])\n",
    "\n",
    "alpha = 0.01\n",
    "n = len(A)\n",
    "\n",
    "t_comp = t_paired(A, B)\n",
    "print(\"statistic = \", t_stat)\n",
    "(t_comp < t.ppf(alpha / 2, n - 1)) or (t_comp > t.ppf(1 - alpha / 2, n - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie mamy więc podstaw do odrzucenia hipotezy zerowej, tj. stwierdzenia, że wytrzymałość nie zależy od metody, na przyjętym poziomie istotności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# możliwe byłoby też wykorzystanie testu na porównanie\n",
    "# dwóch średnich, ale jest on chyba słabszy / nie stosuje\n",
    "# się, gdy zmienne nie są niezależne\n",
    "t_stat(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "print(chi2.ppf(alpha, n))\n",
    "print(chi2.ppf(1 - alpha, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzić, czy rzeczywiście dla dużych n mamy \\$ \\sqrt{\\frac{2nS^2}{\\sigma_0^2}}  \\sim  N(\\sqrt{2n - 3}, 1)\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wysunięto hipotezę, że muzyka przy warsztatach pracy zwiększa wydajność pracy zatrudnionych na pewnym \n",
    "stanowisku roboczym. W celu sprawdzenia tej hipotezy wylosowano grupę 8 robotników i zmierzono im wydajność pracy \n",
    "przed i po umieszczeniu przy ich stanowiskach głośników, z których nadawano cicho muzykę rozrywkową. Wyniki [liczba \n",
    "sztuk na godzinę] przed zainstalowaniem głośników były następujące: 35, 20, 40, 30, 38, 42, 30, 22, a po zainstalowaniu \n",
    "głośników: 36, 24, 52, 46, 44, 50, 40, 48. Zakładając, że wydajność pracy ma rozkład normalny, zweryfikować na poziomie \n",
    "istotności 0,05 hipotezę, że wydajność pracy przy muzyce wzrasta. \n",
    "\n",
    "Wykorzystamy do tego dwa podejścia - najpierw test, który porównuje równość średnich, a potem test sparowany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([35, 20, 40, 30, 38, 42, 30, 22])\n",
    "Y = np.array([36, 24, 52, 46, 44, 50, 40, 48])\n",
    "\n",
    "x_mean, x_std = X.mean(), X.std(ddof=0)\n",
    "y_mean, y_std = Y.mean(), Y.std(ddof=0)\n",
    "\n",
    "alpha = 0.05\n",
    "n = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwsze podejście zaczniemy od testu na równość wariancji \n",
    "\n",
    "$$ H_0 : \\sigma_1^2 = \\sigma_2^2 $$\n",
    "$$ H_1 : \\sigma_1^2 \\neq \\sigma_2^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_stat(s1, s2, n1, n2):\n",
    "    sn, sd, nn, nd = s1, n1, s2, n2\n",
    "    if s1 < s2:\n",
    "        sn, sd, nn, nd = s2, n2, s1, n1\n",
    "        \n",
    "    return (nn / (nn - 1) * sn ** 2) / (nd / (nd - 1) * sd ** 2)\n",
    "\n",
    "f_comp = f_stat(x_std, y_std, n, n)\n",
    " \n",
    "from scipy.stats import f\n",
    "\n",
    "f_comp > f.ppf(1 - alpha / 2, n - 1, n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie odrzucamy więc hipotezy o równości wariancji\n",
    "\n",
    "Badamy więc wartości przeciętne\n",
    "\n",
    "$$ H_0 : \\mu_1 = \\mu_2 $$\n",
    "$$ H_1 : \\mu_1 \\lt \\mu_2 $$\n",
    "\n",
    "obszarem krytycznym jest \\$ (\\infty; t(\\alpha)) \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = n + n - 2\n",
    "t_comp = t_stat(X, Y)\n",
    "t_comp < t.ppf(alpha, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "więc średnie po 'eksperymencie' jest większa - możemy na poziomie istotniości 0.05 stwierdzić, że muzyka zwiększa wydajność pracowników"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Badanie wartości wariancji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finch_beaks = pd.read_csv('data/finch_beaks_2012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finch_beaks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='bdepth', data=finch_beaks, hue='species',\n",
    "            stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postawmy hipotezę dla zmiennej bdepth \\$ H_0: \\sigma_{fortis}^2 = 1\\$ oraz \\$A: \\sigma_{fortis}^2 > 1\\$. Na podstawie histogramu przyjmujemy, że rozkład jest w przybliżeniu normalny.\n",
    "Poziom istotności przyjmujemy \\$ \\alpha = 0.05\\$\n",
    "\n",
    "Dla testów o wartości wariancji posługujemy się statystyką \\$ \\chi^2 = \\frac{nS^2}{\\sigma_0^2}\\$ która przy prawdziwości hipotezy zerowej ma rozkład \\$ \\chi^2\\$ o n - 1 stopniach swobody\n",
    "\n",
    "Przedziałem krytycznym dla testu prawostronnego jest \\$ <\\chi^2(1 - \\alpha, n - 1); \\infty)\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "alpha = 0.05\n",
    "hypothesized_var = 1\n",
    "sample = finch_beaks.loc[finch_beaks['species'] == 'fortis', 'bdepth']\n",
    "n = len(sample)\n",
    "var_f = sample.var()\n",
    "chi2_stat = n * var_f / hypothesized_var\n",
    "\n",
    "chi2_stat > chi2.ppf(1 - alpha, n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie odrzucamy więc hipotezy zerowej "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tablicach podane są kwantyle rozkładu \\$ \\chi^2\\$ zwykle do liczności próby 30/50. Aby obliczyć kwantyle wyższych stopni swobody, możemy posłużyć się zależnością\n",
    "\n",
    "\\$ (\\chi^2)_p = \\frac{1}{2}(\\sqrt{2n - 1} + \\lambda_p)^2 \\$\n",
    "\n",
    "gdzie \\$ \\lambda_p\\$ jest kwantylem rozkładu \\$ N(0, 1) \\$\n",
    "\n",
    "\\$ F(\\lambda_p) = p => 0.5 + \\Phi(\\lambda_p) => \\Phi(\\lambda_p) = p - 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_05 = -1.64\n",
    "\n",
    "chi2_05 = 0.5 * ((np.sqrt(2 * n - 1) + lambda_05) ** 2)\n",
    "print(chi2_05, chi2.ppf(0.05, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "bootstrap = np.empty(k)\n",
    "\n",
    "for i in range(k):\n",
    "    bootstrap[i] = np.var(np.random.choice(sample, size=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = np.sqrt(2 * n * bootstrap / hypothesized_var) - np.sqrt(2 * n - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pewne wyniki pomiarów mają rozkład normalny o nieznanych parametrach. Otrzymano wyniki (niżej). Na poziomie istotności 0.05 zbadać, czy \\$ \\sigma^2 = 0.06 \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([18.17, 18.21, 18.05, 18.14, 18.19, 18.22, 18.06, 18.08])\n",
    "\n",
    "print(x.mean())\n",
    "print(x.var(ddof=0))\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "def chi2_stat(x, hypothesized_var):\n",
    "    n = len(x)\n",
    "    return n * x.var(ddof=0) / hypothesized_var\n",
    "\n",
    "chi2_comp = chi2_stat(x, 0.06)\n",
    "\n",
    "chi2_comp > chi2.ppf(1 - alpha, len(x) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nie mamy podstaw do odrzucenia hipotezy o wartości wariancji równej 0.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Badanie równości wariancji dwóch populacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak to wcześniej zostało wspomniane, do werfikacji równości wariancji korzystamy ze statystyki \n",
    "\n",
    "\\$ F = \\frac{\\frac{n_1}{n_1 - 1}S_1}{\\frac{n_2}{n_2 - 1}S_2}\\$ \n",
    "\n",
    "Która przy prawdziwości hipotezy \\$ H_0\\$ ma rozkład F Snedecora o \\$(n_1 - 1, n_2 - 1)\\$ stopniach swobody \n",
    "\n",
    "\\$ H_0: \\sigma_1^2 = \\sigma_2^2 \\$\n",
    "\n",
    "1. \\$ A: \\sigma_1^2 > \\sigma_2^2\\$\n",
    "\n",
    "wtedy zbiorem krytycznym jest \n",
    "\\$ <F(1 - \\alpha, n_1 - 1, n_2 - 1); +\\infty) \\$\n",
    "\n",
    "2. \\$ A: \\sigma_1^2 < \\sigma_2^2\\$\n",
    "\n",
    "naszą statystyką jest wtedy \\$ F' = \\frac{1}{F} \\$\n",
    "\n",
    "a zbiorem krytycznym jest \n",
    "\\$ <F(1 - \\alpha, n_2 - 1, n_1 - 1); +\\infty) \\$\n",
    "\n",
    "3. \\$ A: \\sigma_1^2 \\neq \\sigma_2^2\\$\n",
    "\n",
    "wtedy obliczamy\n",
    "\n",
    "\\$ F = max(F, F') = \\frac{\\max{S_1^{\\ast2} S_2^{\\ast2}}}{\\min{S_1^{\\ast2} S_2^{\\ast2}}}\\$\n",
    "\n",
    "zbiorem krytycznym jest \\$ <F(1 - \\frac{\\alpha}{2}, n_l - 1, n_m - 1); +\\infty) \\$\n",
    "\n",
    "gdzie \\$ n_l \\$ jest licznością dla której obliczono licznik a \\$ n_m \\$ dla której obliczono mianownik\n",
    "\n",
    "przy obliczaniu kwantyli korzystać możemy z zależności \n",
    "\n",
    "\\$ F_{p, n, m} = \\frac{1}{F_{1 - p, m, n}}\\$\n",
    "\n",
    "Ale jeśli korzystamy z wyżej podanych przedziałów krytycznych, to nie trzeba z tego wzoru korzystać"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postawmy hipotezę dotyczącą równości wariancji dwóch populacji \\$ H_0: \\sigma_1^2 = \\sigma_2^2 \\$, gdzie \\$ A: \\sigma_1^2 \\neq \\sigma_2^2 \\$ na poziomie istotności \\$ \\alpha = 0.05 \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "sample1 = finch_beaks.loc[finch_beaks['species'] == 'fortis', 'bdepth']\n",
    "sample2 = finch_beaks.loc[finch_beaks['species'] == 'scandens', 'bdepth']\n",
    "\n",
    "# ddof=1 domyślnie\n",
    "var1 = np.var(sample1)\n",
    "var2 = np.var(sample2)\n",
    "\n",
    "if var1 >= var2:\n",
    "    F_computed = var1 / var2\n",
    "    nl, nm = n1, n2\n",
    "else:\n",
    "    F_computed = var2 / var1\n",
    "    nl, nm = n2, n1\n",
    "\n",
    "from scipy.stats import f\n",
    "\n",
    "critical_set = f.ppf(1 - alpha/2, nl - 1, nm - 1)\n",
    "F_computed > critical_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie odrzucamy więc hipotezy zerowej o równości wariancji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station = pd.read_csv('data/sheffield_weather_station.csv', \n",
    "                             comment='#', sep='\\s+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Badanie równości wariancji >= 3 populacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby móc użyć testu Bartletta, rozkłady muszą być normalne. \n",
    "W tym celu pobrane zostały próby bootstrapowe i obliczone średnie, bo z centralnego prawa granicznego wynika, że rozkład średnich będzie miał aproksymowany rozkład normalny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data, size=1000, stat_fun=np.mean):\n",
    "    n = len(data)\n",
    "    return np.array([stat_fun(np.random.choice(data, size=n)) for _ in range(size)])\n",
    "\n",
    "\n",
    "bootstrap_season_rain = []\n",
    "for season in range(1, 13):\n",
    "    sample = weather_station.loc[weather_station['mm'] == season, 'rain']\n",
    "    bootstrap_season_rain.append(bootstrap(sample))\n",
    "\n",
    "rows = 6\n",
    "cols = 2\n",
    "fig, axes = plt.subplots(6, 2, figsize=(15, 15))\n",
    "\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        month = r + c * 6\n",
    "        axes[r][c].hist(bootstrap_season_rain[month], label=f\"month={month + 1}\")\n",
    "        axes[r][c].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for season in range(1, 13):\n",
    "    sizes.append(len(weather_station.loc[weather_station['mm'] == season]\n",
    "))\n",
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bez przytaczania wzoru, przy założeniu że próby są równej liczności obliczamy wartości nieobciążonej wariancji dla każdej próby, \\$ S^{\\ast^2}_i, i = 1, ..., k\\$ i statystykę \\$ \\chi^2 \\$ (wzór niżej) która będzie miała dla nawet małych prób ( >= 6) rozkład \\$ \\chi^2 \\$ o \\$ k -1 \\$ stopniach swobody. \n",
    "\n",
    "\\$ H_0: \\sigma^2_1 = \\sigma^2_2 = ... = \\sigma^2_k\\$\n",
    "\n",
    "\\$ A: \\$ dla pewnych \\$ i, j\\$  \\$\\sigma^2_i \\neq \\sigma^2_j \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "s_n = 133\n",
    "k = 12\n",
    "def c(sample_size, k):\n",
    "    if isinstance(sample_size, int):\n",
    "        n = k * sample_size\n",
    "        return 1 + (k + 1) / (3 *(n - k))\n",
    "    else:\n",
    "        n = np.sum(sample_size)\n",
    "        v = np.sum(1 / (sample_size - 1))\n",
    "        return 1 + (v - 1 / (n - k)) / (3 * (k - 1))\n",
    "\n",
    "def chi_compute(sample_size, k, variances):\n",
    "    if isinstance(sample_size, int):\n",
    "        n = sample_size * k\n",
    "    else:    \n",
    "        n = np.sum(sample_size)\n",
    "    v = (n - k) * np.log(np.sum((sample_size - 1) * variances) / (n - k)) - np.sum((sample_size - 1) * np.log(variances)) \n",
    "    return 2.303 * v / c(sample_size, k)\n",
    "\n",
    "variances = [np.var(bootstrap_season_rain[season]) for season in range(12)]\n",
    "chi_computed = chi_compute(np.array(sizes), k, variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "critical_set = chi2.ppf(1 - alpha, k - 1)\n",
    "chi_computed > critical_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Więc odrzucamy hipotezę o równości wariancji "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Badanie równości wskaźnika struktury dwóch populacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy zbadać populacje ze względu na procent populacji posiadający daną cechę. W tym przypadku zakładamy, że badanym rozkładem jest rozkładem dwumianowym. \n",
    "\n",
    "Obie populacje muszą mieć liczności \\$ n_1, n_2 > 100\\$\n",
    "\n",
    "Wtedy, na podstawie integralnego prawa Moivre'a Laplace'a wynika, że zmienna \\$ U = \\frac{\\frac{M}{n} - p}{\\sqrt{\\frac{p(1 - p)}{n}}} \\sim N(0, 1)\\$\n",
    "\n",
    "W naszym przypadku jako statystykę przyjmujemy zmienną \n",
    "\n",
    "\\$ Z = \\frac{\\frac{M_1}{n_1} - \\frac{M_2}{n_2}}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\$\n",
    "\n",
    "\\$ p_0 = \\frac{M_1 + M_2}{n_1 + n_2}\\$\n",
    "\n",
    "\\$ n = \\frac{n_1 n_2}{n_1 + n_2}\\$\n",
    "\n",
    "Więc dla hipotez alternatywnych przedziały krytyczne obliczamy jako\n",
    "1. \\$ A: p_1 \\neq p_2 \\$\n",
    "\n",
    "\\$ (-\\infty; -u(1 - \\frac{\\alpha}{2})> i <u(1 - \\frac{\\alpha}{2}); \\infty)\\$\n",
    "\n",
    "i odpowiednio dla lewostronnego i prawostronnego testu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zakładamy, że w dwóch powiatach przeprowadzono badania w farmach na temat tego, czy dana farma ma konia, czy nie. W powiecie A na 1500 badanych 300 miało konie, w powiecie B na 1800 - 320. Czy proporcje te są równe a fluktuacje są przypadkowe? Poziom ufności to 0.05\n",
    "\n",
    "\\$ H_0: p_1 = p_2 \\$\n",
    "\n",
    "\\$ A: p_1 \\neq p_2 \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "alpha = 0.05\n",
    "n1, n2 = 1500, 1800\n",
    "m1, m2 = 300, 320\n",
    "\n",
    "p1 = m1 / n1\n",
    "p2 = m2 / n2\n",
    "\n",
    "n = (n1 * n2) / (n1 + n2)\n",
    "p_0 = (m1 + m2) / (n1 + n2)\n",
    "\n",
    "Z = (p1 - p2) / np.sqrt(p_0 * (1 - p_0) / n)\n",
    "\n",
    "print(Z, \"is < \", norm.ppf(alpha), \" or >\", norm.ppf(1 - alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie odrzucamy więc hipotezy zerowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test na zgodność \\$ \\chi^2 \\$ Pearson'a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ten bada hipotezę zerową\n",
    "\\$ H_0: F(x) = F_0(x)\\$, która dotyczy rozkładu zmiennej. Postępowanie polega na podziale wartości na \\$ k \\$ klas o końcach przedziału \\$ <a_0, a_1>, ..., <a_k, a_{k + 1}> \\$ i przypisaniu im częstości hipotetyzowanych \\$ p_i = F(a_{i + 1}) - F(a_i)\\$ \n",
    "\n",
    "Wtedy, gdy \\$ n -> \\infty \\$ statystyka \n",
    "\n",
    "\\$ \\chi^2 = \\sum_{i=0}^k \\frac{(n_i - np_i)^2}{np_i} \\$\n",
    "\n",
    "ma rozkład \\$ \\chi^2 \\$ o \\$ k - 1\\$ stopniach swobody\n",
    "\n",
    "Jeżeli parametry \\$ \\theta_1, ..., \\theta_l \\$ od których zależy rozkład nie są znane, to musimy je estymować. Wykorzystuje się wtedy metodę np. największej wiarygodności. Wtedy, dla \\$ n -> \\infty \\$ statystyka ma rozkład \\$ \\chi^2 \\$ o \\$ k - l - 1\\$ stopniach swobody\n",
    "\n",
    "Zalecane jest taki podział na klasy, by prawdopodobieństwa każdej klasy były takie same, tj\n",
    "\\$ p_i = \\frac{1}{k}\\$ \n",
    "\n",
    "Jednak należy znać kwantyle tego rozkładu, gdyż dla i-tej klasy lewą granicę klasy obliczamy jako \n",
    "\\$ F(a_{i}) = \\frac{i}{k}, i = 1, 2, ..., k\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zastosujmy test do weryfikacji hipotezy dotyczącej emisji cząstek \\$ \\alpha \\$, tj. hipotetyzujemy, że cząstki mają rozkład Poissona. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "k = 10\n",
    "l = 1\n",
    "n = 2608\n",
    "particles_emitted = np.arange(k + 1)\n",
    "observed_counts = np.array([ 57, 203, 383, 525, 532, \n",
    "                            408, 273, 139, 45, 27, 16,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako że nie znamy parametru \\$ \\lambda\\$, to będziemy go estymować na podstawie próby. Metodą największej wiarygodności wynika, że estymatorem \\$ \\hat \\lambda = \\overline X \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lmbd = np.sum(observed_counts * particles_emitted) / n\n",
    "lmbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "probabilities = poisson.pmf(particles_emitted, lmbd)\n",
    "theoretical_counts = n * probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(particles_emitted, theoretical_counts, marker='.', linestyle='none', label='theoretical')\n",
    "plt.plot(particles_emitted, observed_counts, marker='*', linestyle='none', label='observed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_computed = np.sum((observed_counts - n * probabilities) ** 2 / (n * probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "przedziałem krytycznym przy poziomie istotności \\$ \\alpha = 0.05 \\$ jest \\$ <\\chi^2(1 - \\alpha, k - l - 1); \\infty)\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "print(chi_computed)\n",
    "\n",
    "chi_computed > chi2.ppf(1 - alpha, k - l - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nie odrzucamy więc hipotezy, że rozkład zmiennej jest rozkładem Poissona. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Kołmogorowa\n",
    "\n",
    "Weryfikujemy hipotezę, że cecha ciągła X ma rozkład określony przez dystrybuantę \\$ F_0(x) \\$. \n",
    "Jako statystykę przyjmujemy\n",
    "\n",
    "$$ D_n = \\sup_x |F_0(x) - S_n(x)|$$\n",
    "\n",
    "gdzie operator sup oznaczna dla zbiorów najmniejszą wartość mniejszą lub równą od wszystkich wartości ze zbioru. \n",
    "\n",
    "\\$ S_n(x) \\$ jest dystrybuantą empiryczną stworzoną na podstawie uszeregowanego zbioru wartości \\$ x_1  \\leq x_2 \\leq ... x_n \\$\n",
    "\n",
    "Warto zauważyć, że statystyka jest miarą rozbieżności między dystrybuantą empiryczną a teoretyczną.\n",
    "\n",
    "Sensem wyboru statystyki \\$ D_n \\$ jest twierdzenie Gliwienki \n",
    "\n",
    "$$ P(\\lim_{n -> \\infty}S_n(x) = F_0(x) | H_0) = 1$$\n",
    "\n",
    "Przy prawdziwości hipotezy \\$ H_0 \\$ statystyka ma rozkład niezależny od przyjętej hipotezy. Posługujemy się kwantylami \\$ P(D_n \\geq d_n(1 - \\alpha)) = \\alpha\\$\n",
    "\n",
    "W praktyce postępujemy tak, że obliczamy granice prawo i lewo-stronne, tj.\n",
    "\n",
    "\\$d_n^- = \\max |F_0(x) - \\frac{i - 1}{n}|\\$\n",
    "\n",
    "\\$d_n^+ = \\max |\\frac{i}{n} - F_0(x)|\\$\n",
    "\n",
    "i wtedy \\$ d_n = max(d_n^-, d_n^+)\\$\n",
    "\n",
    "gdy obliczona statystyka należy do przedziału \\$ <d_n(1 - \\alpha); 1)\\$, to odrzucamy hipotezę zerową. Warto podkreślić, że wszystkie parametry rozkładu muszą być określone.\n",
    "\n",
    "Jeśli natomiast parametry nie są znane, tj \\$ H_0: X \\sim F_0(x, \\theta_1, ... , \\theta_l)\\$, to\n",
    "są one estymowane na podstawie próbki, a korzystamy z granicznej statystyki\n",
    "\n",
    "\\$ P(\\sqrt{n}D_n \\geq \\lambda(1 - \\alpha)) = \\alpha\\$\n",
    "\n",
    "Gdy n jest dostatecznie duże (rzędu kilkaset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hipotetyzujemy, że na przestrzeni lat maksymalne temperatury w styczniu pochodzą z rozkładu normalnego o znanej średniej i wariancji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(month, col):\n",
    "    return pd.to_numeric(weather_station.loc[weather_station['mm'] == month, col], errors='coerce').dropna()\n",
    "\n",
    "max_temps_january = convert_to_numeric(1, 'tmax')\n",
    "years = weather_station.loc[max_temps_january.index, 'yyyy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(years, max_temps_january)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('max temperature')\n",
    "plt.title('Maximum January temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(max_temps_january)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zakładamy, że temperatury pochodzą z rozkładu normalnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "mean_max_temp = max_temps_january.mean()\n",
    "std_max_temp = max_temps_january.std()\n",
    "\n",
    "def ecdf(x):\n",
    "    x = sorted(x)\n",
    "    n = len(x)\n",
    "    k = np.arange(n)\n",
    "    y = k / n\n",
    "    return x, y\n",
    "\n",
    "x, y = ecdf(max_temps_january)\n",
    "plt.plot(x, y, marker='.', linestyle='none', label='empirical')\n",
    "plt.plot(x, norm.cdf(x, mean_max_temp, std_max_temp), marker='.', linestyle='none', label='theoretical')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(max_temps_january)\n",
    "print(n)\n",
    "i = np.arange(1, n + 1)\n",
    "d_n_plus = max(abs((i / n) - norm.cdf(x, mean_max_temp, std_max_temp)))\n",
    "d_n_minus = max(abs(norm.cdf(x, mean_max_temp, std_max_temp) - (i - 1) / n))\n",
    "\n",
    "print(f\"d_n_plus = {d_n_plus}, d_n_minus = {d_n_minus}\")\n",
    "d_n = max(d_n_plus, d_n_minus)\n",
    "d_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ksone\n",
    "\n",
    "alpha = 0.05\n",
    "d_n > ksone.ppf(1 - alpha, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Więc test nie przeczy hipotezie, że rozkład jest normalny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto zauważyć, że tak naprawdę nie znamy parametrów tego rozkładu, więc odpowiednie dla tego testu twierdzenie nie może być spełnione. Więc korzystamy z aproksymowanego rozkładu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstwobign\n",
    "\n",
    "np.sqrt(n) * d_n > kstwobign.ppf(1 - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Porównanie testów Kołmogorowa i \\$ \\chi^2 \\$\n",
    "1. Jeżeli liczność n próbki jest mała (nawet rzędu kilku) i cecha jest ciągła, to należy stosować test Kołmogorowa\n",
    "2. Przy podziale na klasy o tych samych prawdopodobieństwach, liczności >= kilkadziesiąt i cecha jest ciągła, to należy stosować test Kołmogorowa\n",
    "3. Gdy cecha ma rozkład skokowy, stosujemy \\$ \\chi^2 \\$\n",
    "4. Test Kołmogorova jest czulszy na obliczane różnice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Shapiro-Wilka\n",
    "\n",
    "https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
    "\n",
    "(Nie ma rozkładu dla tego testu w scipy, więc już bez wzoru)\n",
    "\n",
    "\\$ H_0 : X \\sim N(\\mu, \\sigma)\\$\n",
    "\n",
    "Jest to test na normalność. Jako przedział krytyczny przyjmujemy \\$ (0; W(\\frac{\\alpha}{2}, n))\\$ i \\$ (W(1 - \\frac{\\alpha}{2}, n); \\infty)\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "alpha = 0.05\n",
    "shapiro(max_temps_january).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Smirova-Kołmogorowa\n",
    "\n",
    "Jest to test badający równość rozkładów dwóch zmiennych. Zakładamy, że mają one dowolny rozkład oraz odpowiednio duże liczności. Postępowanie jest podobne jak w teście Kołmogorova.\n",
    "Statystyką testową jest\n",
    "\n",
    "$$ D_{n1, n2} = \\sup_x |S_{n_1} - S_{n_2}(x)|$$\n",
    "\n",
    "wtedy asymptotycznie statystyka\n",
    "\n",
    "\\$ \\lambda = \\sqrt{n} D_{n1, n2}\\$ gdzie \\$ n = \\frac{n_1n_2}{n_1 + n_2}\\$\n",
    "\n",
    "ma rozkład Kołmogorova\n",
    "\n",
    "Zbiorem krytycznym jest \\$ <\\lambda(1 - \\alpha); \\infty>\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month\n",
    "max_temps_april = convert_to_numeric(4, 'tmax')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# ?? Bo to zparowane wtedy będą \n",
    "# temps = pd.DataFrame(max_temps_january).join(max_temps_april, lsuffix='tmax1', rsuffix='tmax4')\n",
    "ax.hist([max_temps_january, max_temps_april])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = len(max_temps_january)\n",
    "n4 = len(max_temps_april)\n",
    "\n",
    "assert n1 == n4\n",
    "\n",
    "t1, ecdf1 = ecdf(max_temps_january)\n",
    "t4, ecdf4 = ecdf(max_temps_april)\n",
    "\n",
    "D_n = max(abs(ecdf1 - ecdf4))\n",
    "\n",
    "n = n1 * n4 / (n1 + n4)\n",
    "\n",
    "lmbd = np.sqrt(n) * D_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd > kstwobign.ppf(1 - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "więc odrzucamy hipotezę zerową, że rozkłady są takie same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Wilcoxona\n",
    "\n",
    "Jest to test na identyczność rozkładów dwóch populacji. Schemat postępowania jest następujący: oznaczmy przez \\$ x \\$ wartości pierwszego z rozkładów, a przez \\$ y \\$ wartości drugiego. Jeżeli teraz uporządkujemy wartości we wspólny, uporządkowany ciąg \n",
    "\n",
    "$$ x_1 \\leq ... x_{k1} \\leq y_1 \\leq ... y_{k2} \\leq x_{k2 + 1} \\leq ... $$\n",
    "\n",
    "zmienną losową \\$ U_x \\$ jest liczba inwersji elementów \\$ x \\$ ze względu na \\$ y \\$, tj dla konkretnego \\$ x \\$ obliczamy, ile elementów \\$ y \\$ go poprzedza. \n",
    "\n",
    "Można wykazać, że\n",
    "\n",
    "\\$ U_x + U_y = n_1n_2\\$\n",
    "\n",
    "dla \\$ n_1, n_2 \\geq 4, n_1 + n_2 \\geq 20\\$ statystyka \\$ U \\sim N(\\frac{n_1n_2}{2}, \\sqrt{\\frac{n_1n_2(n_1 + n_2 + 1)}{12}})\\$\n",
    "\n",
    "Hipotezę zerową odrzucamy, gdy \\$ u \\leq u_{\\alpha} \\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = pd.read_csv('data/animals.txt', sep='\\t', header=0, names=['gender', 'wrong', 'necessar'])\n",
    "animals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_wrong = animals.loc[animals['gender'] == 1, ['gender', 'wrong']]\n",
    "men_wrong = animals.loc[animals['gender'] == 2, ['gender', 'wrong']]\n",
    "\n",
    "ranking = animals.sort_values(by='wrong').reset_index(drop=True)\n",
    "# .loc[animals['gender'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = (women_wrong < men_wrong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
